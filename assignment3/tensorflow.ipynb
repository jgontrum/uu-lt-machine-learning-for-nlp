{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Tensorflow\n",
    "\n",
    "Implementing a Linear Classifier for polarity movie reviews.\n",
    "\n",
    "See course homepage: http://stp.lingfil.uu.se/~nivre/master/ml.html\n",
    "\n",
    "See assignment: http://stp.lingfil.uu.se/~shaooyan/ml18/Assignment3.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import util\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularisation strength\n",
    "reg_lambda = 0.001\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Number of training iterations\n",
    "niterations = 15\n",
    "\n",
    "# Loss function to use (select one and comment out the other)\n",
    "def logistic_loss(y, pred):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    pred = tf.cast(pred, tf.float32)\n",
    "    return tf.reduce_mean(tf.log(1.0 + tf.exp(-y*pred)))\n",
    "\n",
    "loss_function = logistic_loss\n",
    "\n",
    "# Type of regularisation to use (select one and comment out the other)\n",
    "regulariser = tf.contrib.layers.l1_regularizer(reg_lambda)\n",
    "\n",
    "# This should only be enabled once you've decided on a final set\n",
    "# of hyperparameters\n",
    "enable_test_set_scoring = False\n",
    "\n",
    "# Type of features to use. This can be set to 'bigram' or 'unigram+bigram'\n",
    "# to use bigram features instead of or in addition to unigram features.\n",
    "# Not required for assignment.\n",
    "feature_type = 'unigram'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 50920\n"
     ]
    }
   ],
   "source": [
    "data = util.load_movie_data('poldata.zip')\n",
    "\n",
    "data.select_feature_type(feature_type)\n",
    "\n",
    "# Split the data set randomly into training, validation and test sets.\n",
    "training_data, val_data, test_data = data.train_val_test_split()\n",
    "\n",
    "nfeatures = len(training_data.vocabulary)\n",
    "\n",
    "# Convert the sparse indices into dense vectors\n",
    "ds_training = util.sparse_to_dense(training_data, nfeatures)\n",
    "ds_val = util.sparse_to_dense(val_data, nfeatures)\n",
    "ds_test = util.sparse_to_dense(test_data, nfeatures)\n",
    "\n",
    "print(\"Number of features: %s\" % nfeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope('classifier'):\n",
    "\n",
    "        # Define the placeholder where we feed in the data\n",
    "        features = tf.placeholder(tf.int32, [None, nfeatures],\n",
    "                                  name='input_placeholder')\n",
    "\n",
    "        labels = tf.placeholder(tf.float16, [None], name='labels_placeholder')\n",
    "        \n",
    "        # Variables are what we try to estimate!\n",
    "        # Define the weights of the classifier\n",
    "        weights = tf.get_variable('weights', [nfeatures],\n",
    "                                  initializer=tf.zeros_initializer())\n",
    "        \n",
    "        # The bias is a scalar\n",
    "        bias = tf.get_variable('bias', [],\n",
    "                               initializer=tf.zeros_initializer())\n",
    "\n",
    "        # Two tensors must have same dtype and compatible shape for dot product\n",
    "        features = tf.cast(features, tf.float32)\n",
    "        \n",
    "        exp_weights = tf.reshape(weights, [nfeatures, 1])\n",
    "\n",
    "        # Compute dot product\n",
    "        logits = tf.matmul(features, exp_weights)\n",
    "        \n",
    "        logits = tf.add(logits, bias) # TODO Is this correct?\n",
    "        \n",
    "        # Reshape the result to a vector to remove the dimension\n",
    "        # added to `exp_weights`.\n",
    "        logits = tf.reshape(logits, [-1])\n",
    "        # Define loss\n",
    "\n",
    "        loss_ureg = loss_function(labels, logits)\n",
    "\n",
    "        # Regularisation\n",
    "        # L1_regularisation\n",
    "        loss_reg = regulariser(weights)\n",
    "        loss = loss_ureg + loss_reg\n",
    "\n",
    "        # Configuerate gradient descent\n",
    "        config = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        # Initialiser\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "graph.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 15 iterations...\n",
      "Iteration 0: training_loss_reg 0.693146, training_loss_unreg 0.693146, training_acc 0.6025, val_loss 0.692417, val_acc 0.555\n",
      "Iteration 1: training_loss_reg 0.691708, training_loss_unreg 0.692049, training_acc 0.636875, val_loss 0.691612, val_acc 0.605\n",
      "Iteration 2: training_loss_reg 0.690623, training_loss_unreg 0.691087, training_acc 0.671875, val_loss 0.690977, val_acc 0.63\n",
      "Iteration 3: training_loss_reg 0.689383, training_loss_unreg 0.690157, training_acc 0.696875, val_loss 0.690023, val_acc 0.655\n",
      "Iteration 4: training_loss_reg 0.68824, training_loss_unreg 0.689002, training_acc 0.719375, val_loss 0.689293, val_acc 0.67\n",
      "Iteration 5: training_loss_reg 0.68718, training_loss_unreg 0.688137, training_acc 0.746875, val_loss 0.688529, val_acc 0.71\n",
      "Iteration 6: training_loss_reg 0.685962, training_loss_unreg 0.687096, training_acc 0.759375, val_loss 0.687568, val_acc 0.71\n",
      "Iteration 7: training_loss_reg 0.684915, training_loss_unreg 0.686008, training_acc 0.768125, val_loss 0.686811, val_acc 0.72\n",
      "Iteration 8: training_loss_reg 0.68387, training_loss_unreg 0.685125, training_acc 0.781875, val_loss 0.686162, val_acc 0.735\n",
      "Iteration 9: training_loss_reg 0.682624, training_loss_unreg 0.684176, training_acc 0.795, val_loss 0.685165, val_acc 0.75\n",
      "Iteration 10: training_loss_reg 0.681567, training_loss_unreg 0.683034, training_acc 0.80125, val_loss 0.684516, val_acc 0.75\n",
      "Iteration 11: training_loss_reg 0.680533, training_loss_unreg 0.682267, training_acc 0.815, val_loss 0.683698, val_acc 0.75\n",
      "Iteration 12: training_loss_reg 0.679352, training_loss_unreg 0.681189, training_acc 0.820625, val_loss 0.68277, val_acc 0.75\n",
      "Iteration 13: training_loss_reg 0.678329, training_loss_unreg 0.680139, training_acc 0.828125, val_loss 0.682182, val_acc 0.755\n",
      "Iteration 14: training_loss_reg 0.677235, training_loss_unreg 0.679377, training_acc 0.835625, val_loss 0.681413, val_acc 0.745\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Define a training session and train the classifier\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    def predict(input_features):\n",
    "        \"\"\"Applies the classifier to the data and returns a list of predicted labels.\"\"\"\n",
    "        predictions = []\n",
    "        pred = sess.run(logits, feed_dict={features: input_features})\n",
    "        for x in pred:\n",
    "            if x > 0:\n",
    "                predictions.append(1.0)\n",
    "            else:\n",
    "                predictions.append(-1.0)\n",
    "        return predictions\n",
    "    \n",
    "    def accuracy(gold, hypothesis):\n",
    "        \"\"\"Computes an accuracy score given two vectors of labels.\"\"\"\n",
    "        assert len(gold) == len(hypothesis)\n",
    "        return sum(g == h for g, h in zip(gold, hypothesis)) / len(gold)\n",
    "\n",
    "    # Before starting, initialize the variables. We will 'run' this first.\n",
    "    sess.run(init)\n",
    "    training_log = []\n",
    "    \n",
    "    # Training iterations\n",
    "    print(\"Run %s iterations...\" % niterations)\n",
    "\n",
    "    for i in range(niterations):\n",
    "        _, t_loss_unreg, t_loss_reg = sess.run(\n",
    "            [config, loss_ureg, loss],\n",
    "            feed_dict={features: ds_training, labels: training_data.labels})\n",
    "\n",
    "        v_loss = sess.run(loss, feed_dict={features: ds_val, labels:val_data.labels})\n",
    "\n",
    "        training_predictions = predict(ds_training)\n",
    "        training_accuracy = accuracy(training_data.labels, training_predictions)\n",
    "\n",
    "        val_predictions = predict(ds_val)\n",
    "        val_accuracy = accuracy(val_data.labels, val_predictions)\n",
    "\n",
    "        log_record = collections.OrderedDict()\n",
    "        log_record['training_loss_reg'] = t_loss_unreg\n",
    "        log_record['training_loss_unreg'] = t_loss_reg\n",
    "        log_record['training_acc'] = training_accuracy\n",
    "        log_record['val_loss'] = v_loss\n",
    "        log_record['val_acc'] = val_accuracy\n",
    "\n",
    "        training_log.append(log_record)\n",
    "\n",
    "        # Display info on training progress\n",
    "        util.display_log_record(i, log_record)\n",
    "\n",
    "    print('Training completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0\n",
      "Loss: 2.532714\n",
      "W: -0.360061\n",
      "b: 1.125442\n",
      "\n",
      "Step: 1000\n",
      "Loss: 0.001102\n",
      "W: 0.969438\n",
      "b: -0.926216\n",
      "\n",
      "Step: 2000\n",
      "Loss: 0.000001\n",
      "W: 0.999017\n",
      "b: -0.997628\n",
      "\n",
      "Step: 3000\n",
      "Loss: 0.000000\n",
      "W: 0.999968\n",
      "b: -0.999922\n",
      "\n",
      "Step: 4000\n",
      "Loss: 0.000000\n",
      "W: 0.999996\n",
      "b: -0.999990\n",
      "\n",
      "Step: 5000\n",
      "Loss: 0.000000\n",
      "W: 0.999996\n",
      "b: -0.999990\n",
      "\n",
      "Step: 6000\n",
      "Loss: 0.000000\n",
      "W: 0.999996\n",
      "b: -0.999990\n",
      "\n",
      "Step: 7000\n",
      "Loss: 0.000000\n",
      "W: 0.999996\n",
      "b: -0.999990\n",
      "\n",
      "Step: 8000\n",
      "Loss: 0.000000\n",
      "W: 0.999996\n",
      "b: -0.999990\n",
      "\n",
      "Step: 9000\n",
      "Loss: 0.000000\n",
      "W: 0.999996\n",
      "b: -0.999990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(graph=main_graph)\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(10000):\n",
    "    i_loss, _ = sess.run([loss, config], feed_dict={h_x:x, h_y:y})\n",
    "    if i % 1000 == 0:\n",
    "        print('Step: %d'%i)\n",
    "        print('Loss: %f'%i_loss)\n",
    "        print('W: %f'%sess.run(w))\n",
    "        print('b: %f\\n'%sess.run(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  },
  "notify_time": "30",
  "varInspector": {
   "cols": {
    "lenName": 16.0,
    "lenType": 16.0,
    "lenVar": 40.0
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 434.0,
   "position": {
    "height": "40px",
    "left": "851px",
    "right": "20px",
    "top": "80px",
    "width": "528px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
