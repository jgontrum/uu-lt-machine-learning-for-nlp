{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Tensorflow\n",
    "\n",
    "Implementing a Linear Classifier for polarity movie reviews.\n",
    "\n",
    "See course homepage: http://stp.lingfil.uu.se/~nivre/master/ml.html\n",
    "\n",
    "See assignment: http://stp.lingfil.uu.se/~shaooyan/ml18/Assignment3.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-17T17:35:49.211125Z",
     "start_time": "2018-05-17T17:35:45.630678Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import util\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T10:16:08.668391Z",
     "start_time": "2018-05-18T10:15:18.011052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 50920\n"
     ]
    }
   ],
   "source": [
    "# Type of features to use. This can be set to 'bigram' or 'unigram+bigram'\n",
    "# to use bigram features instead of or in addition to unigram features.\n",
    "# Not required for assignment.\n",
    "feature_type = 'unigram'\n",
    "\n",
    "data = util.load_movie_data('poldata.zip')\n",
    "\n",
    "data.select_feature_type(feature_type)\n",
    "\n",
    "# Split the data set randomly into training, validation and test sets.\n",
    "training_data, val_data, test_data = data.train_val_test_split()\n",
    "\n",
    "nfeatures = len(training_data.vocabulary)\n",
    "\n",
    "# Convert the sparse indices into dense vectors\n",
    "training_X = np.asarray(util.sparse_to_dense(training_data, nfeatures))\n",
    "training_y = np.asarray(training_data.labels)\n",
    "\n",
    "validation_X = np.asarray(util.sparse_to_dense(val_data, nfeatures))\n",
    "validation_y = np.asarray(val_data.labels)\n",
    "\n",
    "test_X = np.asarray(util.sparse_to_dense(test_data, nfeatures))\n",
    "test_y = np.asarray(test_data.labels)\n",
    "\n",
    "print(\"Number of features: %s\" % nfeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to sparse data\n",
    "\n",
    "It is a bit pointless to first create the dense data and then transform it into sparse data,\n",
    "however I didn't want to mess with the internal data structures.\n",
    "\n",
    "See https://www.tensorflow.org/api_docs/python/tf/SparseTensor for a documentation (SparseTensorValue is should be used outside of a Graph context). In the graph, a `sparse_placeholder` has to be used, as well as the `sparse_tensor_dense_matmul` instead of `matmul` to multiply the weights and the features (https://www.tensorflow.org/api_docs/python/tf/sparse_tensor_dense_matmul)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T10:16:23.998878Z",
     "start_time": "2018-05-18T10:16:23.995108Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_indices(dense_matrix):\n",
    "    for r_i, row in enumerate(dense_matrix):\n",
    "        for c_i in np.nditer(np.where(row == 1)):\n",
    "            yield r_i, int(c_i)\n",
    "\n",
    "def dense_to_sparse(dense_matrix):            \n",
    "    indices = list(generate_indices(dense_matrix))\n",
    "    values = [1 for _ in indices]\n",
    "\n",
    "    return tf.SparseTensorValue(indices, values, dense_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T10:16:25.212843Z",
     "start_time": "2018-05-18T10:16:24.725911Z"
    }
   },
   "outputs": [],
   "source": [
    "training_X_sparse = dense_to_sparse(training_X)\n",
    "validation_X_sparse = dense_to_sparse(validation_X)\n",
    "test_X_sparse = dense_to_sparse(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions (Task 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-17T17:36:36.051427Z",
     "start_time": "2018-05-17T17:36:36.046740Z"
    }
   },
   "outputs": [],
   "source": [
    "def logistic_loss(y, pred):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    pred = tf.cast(pred, tf.float32)\n",
    "    return tf.reduce_mean(tf.log(1.0 + tf.exp(-y*pred)))\n",
    "\n",
    "def hinge_loss(y, pred):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    pred = tf.cast(pred, tf.float32)\n",
    "    return tf.losses.hinge_loss(y, pred)\n",
    "\n",
    "def mse_loss(y, pred):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    pred = tf.cast(pred, tf.float32)\n",
    "    return tf.losses.mean_squared_error(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T10:50:34.538143Z",
     "start_time": "2018-05-18T10:50:34.534387Z"
    }
   },
   "outputs": [],
   "source": [
    "# Regularisation strength\n",
    "reg_lambda = 0.001\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Number of training iterations\n",
    "niterations = 15\n",
    "\n",
    "# Number of elements in one batch\n",
    "batch_size = 512\n",
    "\n",
    "# Loss function to use\n",
    "loss_function = logistic_loss\n",
    "loss_function_name = \"logistic_loss\"\n",
    "\n",
    "# Type of regularisation to use (select one and comment out the other)\n",
    "# regulariser = tf.contrib.layers.l2_regularizer(reg_lambda)\n",
    "regulariser = tf.contrib.layers.l1_regularizer(reg_lambda)\n",
    "regulariser_name = \"l1\"\n",
    "\n",
    "# This should only be enabled once you've decided on a final set\n",
    "# of hyperparameters\n",
    "enable_test_set_scoring = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Computational Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T10:50:35.747363Z",
     "start_time": "2018-05-18T10:50:35.676293Z"
    }
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope('classifier'):\n",
    "\n",
    "        # Define the placeholder where we feed in the data\n",
    "        features = tf.sparse_placeholder(tf.int32, [None, nfeatures],\n",
    "                                         name='input_placeholder')\n",
    "\n",
    "        labels = tf.placeholder(tf.int32, [None], name='labels_placeholder')\n",
    "        \n",
    "        # Define the weights of the classifier\n",
    "        weights = tf.get_variable('weights', [nfeatures],\n",
    "                                  initializer=tf.random_normal_initializer())\n",
    "        \n",
    "        # The bias is a scalar\n",
    "        bias = tf.get_variable('bias', [], dtype=tf.float32,\n",
    "                               initializer=tf.random_normal_initializer())\n",
    "\n",
    "        # Two tensors must have same dtype and compatible shape for dot product\n",
    "        features = tf.cast(features, tf.float32)\n",
    "        exp_weights = tf.reshape(weights, [nfeatures, 1])\n",
    "\n",
    "        # Compute dot product and predict\n",
    "        logits = tf.sparse_tensor_dense_matmul(features, exp_weights) + bias\n",
    "\n",
    "        # Reshape the result to a vector to remove the dimension\n",
    "        # added to `exp_weights`.\n",
    "        logits = tf.reshape(logits, [-1])\n",
    "        \n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "        \n",
    "        # Multiply predictions and labels. When a cell is positive,\n",
    "        # the guess was correct. If it was negative, it must be corrected.\n",
    "        a = tf.multiply(logits, labels)\n",
    "        \n",
    "        # Create a vector 'mask' with the length of the batchs\n",
    "        # that has a 1 whereever a predictions was wrong.\n",
    "        mask = tf.map_fn(lambda x: tf.sign(tf.minimum(x, 0) * -1), a)\n",
    "        \n",
    "        # Now create new weights by multiplying labels and the features.\n",
    "        # Also multiply the mask with the result to 'remove' all entries\n",
    "        # for already correctly predicted instances.\n",
    "        fm = tf.sparse_transpose(features) * mask * labels\n",
    "        \n",
    "        # Since sparse tensors dont support the 'reduce_mean' operation,\n",
    "        # we have to simulate it by dividing by the number of instances.\n",
    "        divider = tf.cast(tf.fill([nfeatures], batch_size), tf.float32)\n",
    "        \n",
    "        # Sum up the differences for all instances and add them to the weights.\n",
    "        weights = weights + tf.sparse_reduce_sum(fm, 1) / divider \n",
    "        \n",
    "        # Now also update the bias in a similar way.\n",
    "        bias = bias + tf.reduce_mean(labels * mask)\n",
    "        \n",
    "        # Calculate the loss so we know if we are getting better.\n",
    "        loss_ureg = loss_function(labels, logits)\n",
    "        \n",
    "        # Initialiser\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "graph.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T10:51:46.176005Z",
     "start_time": "2018-05-18T10:50:43.727757Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 15 iterations...\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "0.5175\n",
      "<function l1_regularizer.<locals>.l1 at 0x7f031b3c8730>\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Define a training session and train the classifier\n",
    "sess = tf.Session(graph=graph)\n",
    "\n",
    "def predict(input_features):\n",
    "    \"\"\"Applies the classifier to the data and returns a list of predicted labels.\"\"\"\n",
    "    predictions = []\n",
    "    pred = sess.run(logits, feed_dict={features: input_features})\n",
    "    for x in pred:\n",
    "        if x > 0:\n",
    "            predictions.append(1.0)\n",
    "        else:\n",
    "            predictions.append(-1.0)\n",
    "    return predictions\n",
    "\n",
    "def accuracy(gold, hypothesis):\n",
    "    \"\"\"Computes an accuracy score given two vectors of labels.\"\"\"\n",
    "    assert len(gold) == len(hypothesis)\n",
    "    return sum(g == h for g, h in zip(gold, hypothesis)) / len(gold)\n",
    "\n",
    "# Before starting, initialize the variables. We will 'run' this first.\n",
    "sess.run(init)\n",
    "\n",
    "# Training iterations\n",
    "print(\"Run %s iterations...\" % niterations)\n",
    "\n",
    "# Set up containers to collect logs\n",
    "stats = {\n",
    "    \"training_loss_reg\": [],\n",
    "    \"training_loss_unreg\": [],\n",
    "    \"training_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "training_log = []\n",
    "for i in range(50):\n",
    "    num_instances = training_X.shape[0]\n",
    "    \n",
    "    # Shuffle the data using a random numpy index array\n",
    "    permutation = np.random.permutation(num_instances)\n",
    "    shuffled_X = training_X[permutation]\n",
    "    shuffled_y = training_y[permutation]\n",
    "    \n",
    "    # Now create batches for both\n",
    "    for position in range(0, num_instances - 1, batch_size):\n",
    "        # Make sure that the last batch has an apropriate size\n",
    "        actual_batch_size = batch_size if position + batch_size < num_instances \\\n",
    "            else position + batch_size - num_instances\n",
    "        \n",
    "        batch_X = shuffled_X[position:position + actual_batch_size]\n",
    "        batch_y = shuffled_y[position:position + actual_batch_size]\n",
    "        \n",
    "        # Create dense tensor for the features\n",
    "        sparse_X = dense_to_sparse(batch_X)\n",
    "        \n",
    "        b, w, loss_val = sess.run(\n",
    "            [bias, weights, loss_ureg],\n",
    "            feed_dict={features: sparse_X, labels: batch_y})\n",
    "#         print(\"LOSS\", loss_val)\n",
    "\n",
    "    training_predictions = predict(training_X_sparse)\n",
    "    training_accuracy = accuracy(training_y, training_predictions)\n",
    "    print(training_accuracy)\n",
    "    \n",
    "print(regulariser)\n",
    "print('Training completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-18T09:06:58.331873Z",
     "start_time": "2018-05-18T09:06:58.310555Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Empty 'DataFrame': no numeric data to plot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-8cd80561c9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'graphs_%s.pdf'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/assignment3/env/lib64/python3.4/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[1;32m   2625\u001b[0m                           \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2626\u001b[0m                           \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecondary_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2627\u001b[0;31m                           sort_columns=sort_columns, **kwds)\n\u001b[0m\u001b[1;32m   2628\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/assignment3/env/lib64/python3.4/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36mplot_frame\u001b[0;34m(data, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[1;32m   1867\u001b[0m                  \u001b[0myerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1868\u001b[0m                  \u001b[0msecondary_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1869\u001b[0;31m                  **kwds)\n\u001b[0m\u001b[1;32m   1870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/assignment3/env/lib64/python3.4/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_plot\u001b[0;34m(data, x, y, subplots, ax, kind, **kwds)\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0mplot_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1694\u001b[0;31m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1695\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1696\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/assignment3/env/lib64/python3.4/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ml/assignment3/env/lib64/python3.4/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m_compute_plot_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             raise TypeError('Empty {0!r}: no numeric data to '\n\u001b[0;32m--> 352\u001b[0;31m                             'plot'.format(numeric_data.__class__.__name__))\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumeric_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Empty 'DataFrame': no numeric data to plot"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(stats)\n",
    "\n",
    "df.plot(subplots=True, layout=(-1, 2), figsize=(20, 25), sharex=False, sharey=False)\n",
    "\n",
    "plt.savefig('graphs_%s.pdf' % time.time())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-17T17:09:20.844011Z",
     "start_time": "2018-05-17T17:09:20.495140Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "MODEL CHARACTERISTICS\n",
      "=====================\n",
      "\n",
      "\n",
      "Data set: Movie reviews - Regulariser: 0.001 - Learning rate: 0.001\n",
      "\n",
      "Best regularised training loss: 0.691499\n",
      "Final regularised training loss: 0.691499\n",
      "Best validation loss: 0.691958\n",
      "Final validation loss: 0.691958\n",
      "\n",
      "Number of weights: 50920\n",
      "Bias: 2.77394e-05\n",
      "Number of weights with magnitude > 0.01: 0\n",
      "\n",
      "Top 1 positive features:\n",
      "0.000562158\tlife\n",
      "\n",
      "Top 1 negative features:\n",
      "-0.000954029\tbad\n"
     ]
    }
   ],
   "source": [
    "print('=====================')\n",
    "print('MODEL CHARACTERISTICS')\n",
    "print('=====================')\n",
    "print()\n",
    "\n",
    "# Display some useful statistics about the model and the training process.\n",
    "title = 'Data set: %s - Regulariser: %g - Learning rate: %g' % (data.name, reg_lambda, learning_rate)\n",
    "\n",
    "print()\n",
    "\n",
    "final_weights = sess.run(weights)\n",
    "final_bias = sess.run(bias)\n",
    "util.show_stats(title, training_log, final_weights, final_bias, data.vocabulary,\n",
    "                top_n=1, write_to_file=\"results.csv\", configuration={\n",
    "                    'reg_lambda': reg_lambda,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'loss_function': loss_function_name,\n",
    "                    'regulariser': regulariser_name,\n",
    "                    'niterations': niterations,\n",
    "                    'val_accuracy': val_accuracy\n",
    "                })\n",
    "\n",
    "\n",
    "# util.create_plots(title, training_log, weights, log_keys=['training_loss_reg', 'val_loss'])\n",
    "\n",
    "if enable_test_set_scoring:\n",
    "    # Check the performance on the test set.\n",
    "    test_loss = sess.run(loss, feed_dict={features: ds_test, labels:test_data.labels})\n",
    "    test_predictions = predict(ds_test)\n",
    "    test_accuracy = accuracy(test_data.labels, test_predictions)\n",
    "\n",
    "    print()\n",
    "    print('====================')\n",
    "    print('TEST SET PERFORMANCE')\n",
    "    print('====================')\n",
    "    print()\n",
    "    print('Test loss: %g' % test_loss)\n",
    "    print('Test accuracy: %g' % test_accuracy)\n",
    "    \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "\n",
    "Run with standard parameters.\n",
    "\n",
    "Top 10 positive words:\n",
    "\n",
    "1. life\n",
    "2. also\n",
    "3. best\n",
    "4. world\n",
    "5. many\n",
    "6. both\n",
    "7. perfect\n",
    "8. performances\n",
    "9. very\n",
    "10. great\n",
    "\n",
    "Top 10 negative words:\n",
    "bad\n",
    "1. worst\n",
    "2. plot\n",
    "3. stupid\n",
    "4. ?\n",
    "5. boring\n",
    "6. script\n",
    "7. nothing\n",
    "8. why\n",
    "9. least\n",
    "10. !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  },
  "notify_time": "10",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 434,
   "position": {
    "height": "40px",
    "left": "851px",
    "right": "20px",
    "top": "80px",
    "width": "528px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
